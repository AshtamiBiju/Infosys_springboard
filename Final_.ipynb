{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUk7WJacIZnd",
    "outputId": "d346421d-455e-4d6e-eef8-4dd05b975275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries installed and imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# 0. INSTALL AND IMPORT LIBRARIES\n",
    "# ========================\n",
    "!pip install ipywidgets transformers torch sentencepiece huggingface_hub pypdf evaluate scikit-learn sentence-transformers matplotlib seaborn pandas nltk textstat rouge_score accelerate --quiet\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import textstat\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import evaluate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"✅ Libraries installed and imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348,
     "referenced_widgets": [
      "e923c12727c84c65b5a52f1ed603c1cd",
      "4a9b74b8d2c9457e83fc5e7746b7869f",
      "4b94fc5585c14a99ab3ad3f5ca53db33",
      "8e11ed9b85644eb1916b93680e9ca8d4",
      "d35945e0c93a496b8b58792ad46344ec",
      "eef5ae8cae8f439f9dfa6d69ad48343e",
      "ccb8b34408774de892abefc0247211be",
      "2cab2ec90584432d922c679d54e38d31",
      "fa10a8ff959b4c3c953dc5ef360058b0",
      "1dc0c0ef73ee4d5c96dd7f35845bfa5e",
      "a0eb6ab8b1a94b3d80e06470efa41011",
      "c8d2b4ed19894cc88694b417ea1f22a4",
      "c2f8a528210c4ce5b7154d0675c3306b",
      "e8e499a4af7d48fcbc94c744a3e21ee0",
      "f5cb6387e9ae4c9b94db8345f0fd7cae",
      "3c0dabab492143169006bce554293006",
      "045959e5e299446b935a82387d72b5b6"
     ]
    },
    "id": "drrEa6a5PPVd",
    "outputId": "eeeaa4b8-b9cd-4400-eaa0-2b67ce959d2e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e923c12727c84c65b5a52f1ed603c1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hugging Face Hub login successful.\n"
     ]
    }
   ],
   "source": [
    "# 1. HUGGING FACE AUTHENTICATION\n",
    "# ========================\n",
    "try:\n",
    "    HF_TOKEN = os.environ.get('HF_TOKEN')  # Or Colab secret\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"✅ Hugging Face Hub login successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"🛑 Hugging Face login failed. Ensure HF_TOKEN is set.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ekhHjh9zPTkT"
   },
   "outputs": [],
   "source": [
    "# 2. DOWNLOAD NLTK DATA\n",
    "# ========================\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489,
     "referenced_widgets": [
      "d7831a7a7b2f4571aed0ebec7ce31069",
      "ff84b9b4865147f49437dfb27257bc63",
      "310c1c2c62094747958e0ad066f89027",
      "bd808c3182644ba0970173dcdd06c059",
      "dc68271c38ac40b18a5bd93a70a9895f",
      "30e845b2a1d447859f8ef0477095a2be",
      "e7938185728e4305be80dd87124d834a",
      "53105ed9087142d3b2aff338493f16fa",
      "e8a8894b5c3a4e62b788935f0b0a944a",
      "3e8af7e78bf940f3b2fd85ef9618d70c",
      "70b797f2797448a18130caed99e994ab",
      "9b9cc2f5a1914f7a969586b5477737a2",
      "6cde3214471a4037bd4831291185123d",
      "2ee0f6ded98e4a0db4d2dec4b6388254",
      "7698ab04955a46b39344b72bb8c7b22c",
      "deda71bfdfa443a9b11b76fd839d880b",
      "e9a70a0a939e4c90a6b8ff594e302c4a",
      "58868874e0bc433a8994f2f7d03f5570",
      "647fd938ae674016802015862994350d",
      "706b8624f20448839bd20a1685f83a5f",
      "8e9eaa017a9041d8b72636dbecaf6959",
      "775d1a256d05418195c8a2dd50fb7df6"
     ]
    },
    "id": "-EMy40miPXP4",
    "outputId": "f5f59a37-87c4-440b-f4f3-51a7403a0ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7831a7a7b2f4571aed0ebec7ce31069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9cc2f5a1914f7a969586b5477737a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1371917501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Model with automatic CPU/GPU split and offloading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m model_gemma = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mmodel_id_gemma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5278\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5279\u001b[0;31m                 \u001b[0mdispatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdevice_map_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    505\u001b[0m                 \u001b[0;34m\"You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead."
     ]
    }
   ],
   "source": [
    "# 3. LOAD MODELS\n",
    "# ========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "MODELS = {}\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# TinyLlama\n",
    "model_id_tiny = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MODELS['tinyllama'] = {'tokenizer': AutoTokenizer.from_pretrained(model_id_tiny),\n",
    "                       'model': AutoModelForCausalLM.from_pretrained(model_id_tiny, torch_dtype=torch.bfloat16, device_map=\"auto\"),\n",
    "                       'name': \"TinyLlama-1.1B-Chat\"}\n",
    "\n",
    "# Phi\n",
    "model_id_phi = \"microsoft/phi-2\"\n",
    "MODELS['phi'] = {'tokenizer': AutoTokenizer.from_pretrained(model_id_phi, trust_remote_code=True),\n",
    "                 'model': AutoModelForCausalLM.from_pretrained(model_id_phi, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True),\n",
    "                 'name': \"Phi-2\"}\n",
    "\n",
    "# BART\n",
    "model_id_bart = \"facebook/bart-large-cnn\"\n",
    "MODELS['bart'] = {'summarizer': pipeline(\"summarization\", model=model_id_bart, device=0 if device==\"cuda\" else -1),\n",
    "                  'name': 'BART-Large-CNN'}\n",
    "\n",
    "# Gemma\n",
    "model_id_gemma = \"google/gemma-2b-it\"\n",
    "offload_dir = \"gemma_offload\"  # Folder to offload layers to disk\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(model_id_gemma, token=HF_TOKEN)\n",
    "\n",
    "# Model with automatic CPU/GPU split and offloading\n",
    "model_gemma = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_gemma,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",           # <-- let Hugging Face handle CPU/GPU split\n",
    "    offload_folder=offload_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "MODELS['gemma'] = {\n",
    "    'tokenizer': tokenizer_gemma,\n",
    "    'model': model_gemma,\n",
    "    'name': \"Gemma-2B-IT\"\n",
    "}\n",
    "\n",
    "print(\"✅ Gemma loaded with CPU/GPU auto split successfully!\")\n",
    "\n",
    "\n",
    "# TextRank embeddings\n",
    "MODELS['embedding'] = {'model': SentenceTransformer('all-MiniLM-L6-v2', device=device),\n",
    "                       'name': 'TextRank (Embeddings)'}\n",
    "\n",
    "print(\"🎉 All models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aUgaNFePdU6"
   },
   "outputs": [],
   "source": [
    "# 4. SUMMARIZATION FUNCTIONS\n",
    "# ========================\n",
    "def generate_with_chat_template_model(model_key, prompt, max_new_tokens=250):\n",
    "    tokenizer = MODELS[model_key]['tokenizer']\n",
    "    model = MODELS[model_key]['model']\n",
    "\n",
    "    # Some tokenizers may not have apply_chat_template; fallback to simple prompt\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        formatted_prompt = prompt\n",
    "\n",
    "    inputs = tokenizer.encode(formatted_prompt, add_special_tokens=True, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "\n",
    "def generate_with_instruct_model(model_key, prompt, max_new_tokens=250):\n",
    "    tokenizer = MODELS[model_key]['tokenizer']\n",
    "    model = MODELS[model_key]['model']\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    if \"Output:\" in decoded_output:\n",
    "        return decoded_output.split(\"Output:\")[1].strip()\n",
    "    return decoded_output\n",
    "\n",
    "def summarize_abstractive(text, model_key, max_len=200, min_len=50):\n",
    "    if model_key == 'tinyllama':\n",
    "        prompt = f\"Provide a concise, abstractive summary of the following text:\\n\\n{text[:2000]}\"\n",
    "        return generate_with_chat_template_model(model_key, prompt, max_new_tokens=max_len)\n",
    "    elif model_key == 'phi':\n",
    "        prompt = f\"Instruct: Summarize the following text concisely.\\n{text[:2000]}\\nOutput:\"\n",
    "        return generate_with_instruct_model(model_key, prompt, max_new_tokens=max_len)\n",
    "    elif model_key == 'bart':\n",
    "        return MODELS['bart']['summarizer'](text[:4096], max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
    "    return \"Unsupported model.\"\n",
    "\n",
    "def summarize_extractive(text, model_key, num_sentences=5):\n",
    "    if model_key in ['tinyllama', 'phi']:\n",
    "        return summarize_abstractive(text, model_key, max_len=num_sentences*50)\n",
    "    elif model_key == 'embedding':\n",
    "        try:\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            if len(sentences) <= num_sentences: return \"\\n\".join(sentences)\n",
    "            embeddings = MODELS['embedding']['model'].encode(sentences, convert_to_tensor=True)\n",
    "            sim_matrix = cosine_similarity(embeddings.cpu().numpy())\n",
    "            graph = nx.from_numpy_array(sim_matrix)\n",
    "            scores = nx.pagerank(graph)\n",
    "            ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "            return \"\\n\".join([s for _, s in ranked_sentences[:num_sentences]])\n",
    "        except Exception as e:\n",
    "            return f\"TextRank failed: {e}\"\n",
    "    return \"Unsupported model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bg3-tv7RPnMz"
   },
   "outputs": [],
   "source": [
    "# 5. METRICS AND VISUALIZATION\n",
    "# ========================\n",
    "def calculate_metrics(summary, original_text, processing_time):\n",
    "    embedding_model = MODELS['embedding']['model']\n",
    "    embeddings = embedding_model.encode([original_text, summary])\n",
    "    semantic_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    rouge_scores = rouge_metric.compute(predictions=[summary], references=[original_text])\n",
    "    return {\n",
    "        \"ROUGE-1\": round(rouge_scores['rouge1'], 3),\n",
    "        \"ROUGE-2\": round(rouge_scores['rouge2'], 3),\n",
    "        \"Semantic Similarity\": round(semantic_sim, 3),\n",
    "        \"Readability\": round(textstat.flesch_reading_ease(summary), 2),\n",
    "        \"Length (words)\": len(summary.split()),\n",
    "        \"Time (sec)\": round(processing_time, 2),\n",
    "        \"Compression\": f\"{(1 - (len(summary.split()) / len(original_text.split()))) * 100:.1f}%\" if len(original_text.split()) > 0 else \"N/A\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RK42BuIhPtEh"
   },
   "outputs": [],
   "source": [
    "# 6. VISUALIZATION FUNCTIONS\n",
    "# ========================\n",
    "def create_bar_charts(metrics_df):\n",
    "    if metrics_df.empty: return None\n",
    "    df_sorted = metrics_df.sort_values(by=['Model']).reset_index(drop=True)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    fig.suptitle('📊 Model Performance Comparison', fontsize=18, fontweight='bold')\n",
    "    colors = sns.color_palette(\"tab10\", n_colors=len(df_sorted['Model'].unique()))\n",
    "\n",
    "    sns.barplot(data=df_sorted, x='Model', y='ROUGE-1', ax=axes[0], palette=colors).tick_params(axis='x', rotation=45)\n",
    "    axes[0].set_title('ROUGE-1 Score', fontsize=14, fontweight='bold')\n",
    "\n",
    "    sns.barplot(data=df_sorted, x='Model', y='Semantic Similarity', ax=axes[1], palette=colors).tick_params(axis='x', rotation=45)\n",
    "    axes[1].set_title('Semantic Similarity', fontsize=14, fontweight='bold')\n",
    "\n",
    "    sns.barplot(data=df_sorted, x='Model', y='Time (sec)', ax=axes[2], palette=colors).tick_params(axis='x', rotation=45)\n",
    "    axes[2].set_title('Processing Time (seconds)', fontsize=14, fontweight='bold')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_facecolor('#f9f9f9')\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    return fig\n",
    "\n",
    "def create_radar_chart(metrics_df):\n",
    "    if metrics_df.empty: return None\n",
    "    metrics_to_plot = ['ROUGE-1', 'Semantic Similarity', 'Readability']\n",
    "    df_radar = metrics_df.copy()\n",
    "    df_radar['Readability'] = np.clip(df_radar['Readability'] / 100.0, 0, 1)\n",
    "    df_avg = df_radar.groupby('Model')[metrics_to_plot].mean().reset_index()\n",
    "\n",
    "    labels, num_vars = df_avg.columns[1:], len(df_avg.columns[1:])\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist() + [0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    colors = sns.color_palette(\"Set2\", n_colors=len(df_avg))\n",
    "\n",
    "    for i, row in df_avg.iterrows():\n",
    "        values = row.drop('Model').tolist() + [row.drop('Model').tolist()[0]]\n",
    "        ax.plot(angles, values, label=row['Model'], linewidth=2.5, color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.3, color=colors[i])\n",
    "\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels, fontsize=12, fontweight='bold')\n",
    "    ax.grid(color='gray', linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    ax.set_title(\"✨ Multi-Metric Model Comparison\", size=16, fontweight='bold', y=1.1)\n",
    "    return fig\n",
    "\n",
    "print(\"✅ Backend ready: Summarization, metrics, visualizations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot8kRQ5UNhLp"
   },
   "outputs": [],
   "source": [
    "# 7. INTERACTIVE UI SECTIONS\n",
    "# ========================\n",
    "# Here you can reuse your previous UI code for:\n",
    "# - Section A: Summarize with All Models\n",
    "# - Section B: Summarize with Specific Models\n",
    "# Just connect them with these new plot functions and updated MODELS dictionary\n",
    "# 8. INTERACTIVE UI - SECTION A: ALL MODELS\n",
    "# ========================\n",
    "\n",
    "metrics_history_all = []\n",
    "\n",
    "# Model choices\n",
    "abstractive_models = [k for k in ['tinyllama','phi','bart','gemma']]\n",
    "extractive_models = [k for k in ['tinyllama','phi','gemma','embedding']]\n",
    "\n",
    "header_all = widgets.HTML(\"<h2>Section A: Summarize with All Models</h2><p>Paste text or load dataset. All compatible models will run automatically.</p>\")\n",
    "text_input_all = widgets.Textarea(placeholder=\"Paste your text here...\", layout={'width':'99%','height':'180px'})\n",
    "summary_type_all = widgets.RadioButtons(options=['Abstractive','Extractive'], value='Abstractive', description='Type:')\n",
    "generate_button_all = widgets.Button(description='🚀 Generate All Summaries', button_style='primary')\n",
    "clear_button_all = widgets.Button(description='🧹 Clear Outputs', button_style='warning')\n",
    "load_dataset_button = widgets.FileUpload(accept=\".txt,.csv\", multiple=False, description=\"📂 Load External Text\")\n",
    "\n",
    "summary_output_all = widgets.Output(layout={'height':'400px','border':'1px solid #ccc','padding':'10px','overflow':'scroll'})\n",
    "metrics_table_output_all = widgets.Output()\n",
    "bar_plot_output_all = widgets.Output()\n",
    "radar_plot_output_all = widgets.Output()\n",
    "output_accordion_all = widgets.Accordion(children=[metrics_table_output_all, bar_plot_output_all, radar_plot_output_all])\n",
    "output_accordion_all.set_title(0,'📊 Metrics Table')\n",
    "output_accordion_all.set_title(1,'📈 Bar Charts')\n",
    "output_accordion_all.set_title(2,'✨ Radar Plot')\n",
    "\n",
    "def get_text_from_file(uploaded_file):\n",
    "    try:\n",
    "        for name, file_info in uploaded_file.items():\n",
    "            if name.endswith(\".txt\"):\n",
    "                return file_info['content'].decode(\"utf-8\")\n",
    "            elif name.endswith(\".csv\"):\n",
    "                df = pd.read_csv(io.BytesIO(file_info['content']))\n",
    "                return \" \".join(df.iloc[:,0].astype(str).tolist())\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def on_generate_all(b):\n",
    "    generate_button_all.disabled = True\n",
    "    generate_button_all.description = \"Processing...\"\n",
    "    text = text_input_all.value.strip()\n",
    "    if not text:\n",
    "        with summary_output_all: print(\"⚠️ Please paste text or load dataset.\")\n",
    "        generate_button_all.disabled = False\n",
    "        generate_button_all.description = \"🚀 Generate All Summaries\"\n",
    "        return\n",
    "    s_type = summary_type_all.value\n",
    "    model_keys = abstractive_models if s_type=='Abstractive' else extractive_models\n",
    "\n",
    "    with summary_output_all: summary_output_all.clear_output()\n",
    "    with summary_output_all: display(HTML(f\"<hr><h3>Processing Text ({s_type})</h3>\"))\n",
    "\n",
    "    for mk in model_keys:\n",
    "        model_name = MODELS[mk]['name']\n",
    "        with summary_output_all: print(f\"⏳ Summarizing with {model_name}...\")\n",
    "        start = time.time()\n",
    "        summary = summarize_abstractive(text, mk) if s_type=='Abstractive' else summarize_extractive(text, mk)\n",
    "        elapsed = time.time() - start\n",
    "        with summary_output_all: display(HTML(f\"<h4>{model_name}</h4><p>{summary}</p>\"))\n",
    "        metrics = calculate_metrics(summary, text, elapsed)\n",
    "        metrics.update({'Model': model_name,'File':'Pasted Text','Type':s_type})\n",
    "        metrics_history_all.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(metrics_history_all)\n",
    "    with metrics_table_output_all: metrics_table_output_all.clear_output(wait=True); display(df)\n",
    "    with bar_plot_output_all: bar_plot_output_all.clear_output(wait=True); display(create_bar_charts(df))\n",
    "    with radar_plot_output_all: radar_plot_output_all.clear_output(wait=True); display(create_radar_chart(df))\n",
    "    generate_button_all.disabled = False\n",
    "    generate_button_all.description = \"🚀 Generate All Summaries\"\n",
    "\n",
    "def on_clear_all(b):\n",
    "    global metrics_history_all\n",
    "    metrics_history_all=[]\n",
    "    summary_output_all.clear_output()\n",
    "    metrics_table_output_all.clear_output()\n",
    "    bar_plot_output_all.clear_output()\n",
    "    radar_plot_output_all.clear_output()\n",
    "    text_input_all.value=\"\"\n",
    "\n",
    "def on_file_upload(change):\n",
    "    text_input_all.value = get_text_from_file(change['new'])\n",
    "\n",
    "generate_button_all.on_click(on_generate_all)\n",
    "clear_button_all.on_click(on_clear_all)\n",
    "load_dataset_button.observe(on_file_upload, names='value')\n",
    "\n",
    "input_box_all = widgets.VBox([text_input_all, summary_type_all, widgets.HBox([generate_button_all, clear_button_all]), load_dataset_button], layout=widgets.Layout(width='35%', padding='10px', border='1px solid lightgrey', border_radius='5px'))\n",
    "output_box_all = widgets.VBox([summary_output_all, output_accordion_all], layout=widgets.Layout(width='65%', padding='10px'))\n",
    "app_all = widgets.VBox([header_all, widgets.HBox([input_box_all, output_box_all])])\n",
    "display(app_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIVzRR6aN2Q7"
   },
   "outputs": [],
   "source": [
    "# 9. INTERACTIVE UI - SECTION B: SPECIFIC MODELS\n",
    "# ========================\n",
    "\n",
    "metrics_history_specific = []\n",
    "checkboxes = {}\n",
    "\n",
    "header_specific = widgets.HTML(\"<h2>Section B: Summarize with Specific Models</h2><p>Paste text, choose summary type, select models.</p>\")\n",
    "text_input_specific = widgets.Textarea(placeholder=\"Paste text here...\", layout={'width':'99%','height':'180px'})\n",
    "summary_type_specific = widgets.RadioButtons(options=['Abstractive','Extractive'], value='Abstractive')\n",
    "model_checkboxes_out = widgets.Output()\n",
    "generate_button_specific = widgets.Button(description='🚀 Generate Summaries', button_style='primary')\n",
    "clear_button_specific = widgets.Button(description='🧹 Clear Outputs', button_style='warning')\n",
    "\n",
    "summary_output_specific = widgets.Output(layout={'height':'400px','border':'1px solid #ccc','padding':'10px','overflow':'scroll'})\n",
    "metrics_table_output_specific = widgets.Output()\n",
    "bar_plot_output_specific = widgets.Output()\n",
    "radar_plot_output_specific = widgets.Output()\n",
    "output_accordion_specific = widgets.Accordion(children=[metrics_table_output_specific, bar_plot_output_specific, radar_plot_output_specific])\n",
    "output_accordion_specific.set_title(0,'📊 Metrics Table')\n",
    "output_accordion_specific.set_title(1,'📈 Bar Charts')\n",
    "output_accordion_specific.set_title(2,'✨ Radar Plot')\n",
    "\n",
    "def update_checkboxes(s_type):\n",
    "    global checkboxes\n",
    "    checkboxes={}\n",
    "    choices = abstractive_models if s_type=='Abstractive' else extractive_models\n",
    "    with model_checkboxes_out:\n",
    "        model_checkboxes_out.clear_output(wait=True)\n",
    "        for mk in choices:\n",
    "            checkboxes[mk] = widgets.Checkbox(value=True, description=MODELS[mk]['name'])\n",
    "        display(widgets.VBox(list(checkboxes.values())))\n",
    "\n",
    "def on_generate_specific(b):\n",
    "    generate_button_specific.disabled=True\n",
    "    generate_button_specific.description=\"Processing...\"\n",
    "    text = text_input_specific.value.strip()\n",
    "    s_type = summary_type_specific.value\n",
    "    selected_keys = [k for k, cb in checkboxes.items() if cb.value]\n",
    "    if not text or not selected_keys:\n",
    "        with summary_output_specific: print(\"⚠️ Please paste text and select at least one model.\")\n",
    "        generate_button_specific.disabled=False\n",
    "        generate_button_specific.description=\"🚀 Generate Summaries\"\n",
    "        return\n",
    "    with summary_output_specific: summary_output_specific.clear_output()\n",
    "    with summary_output_specific: display(HTML(f\"<hr><h3>Processing Text ({s_type})</h3>\"))\n",
    "    for mk in selected_keys:\n",
    "        model_name = MODELS[mk]['name']\n",
    "        with summary_output_specific: print(f\"⏳ Summarizing with {model_name}...\")\n",
    "        start = time.time()\n",
    "        summary = summarize_abstractive(text, mk) if s_type=='Abstractive' else summarize_extractive(text, mk)\n",
    "        elapsed = time.time()-start\n",
    "        with summary_output_specific: display(HTML(f\"<h4>{model_name}</h4><p>{summary}</p>\"))\n",
    "        metrics = calculate_metrics(summary, text, elapsed)\n",
    "        metrics.update({'Model':model_name,'File':'Pasted Text','Type':s_type})\n",
    "        metrics_history_specific.append(metrics)\n",
    "    df = pd.DataFrame(metrics_history_specific)\n",
    "    with metrics_table_output_specific: metrics_table_output_specific.clear_output(wait=True); display(df)\n",
    "    with bar_plot_output_specific: bar_plot_output_specific.clear_output(wait=True); display(create_bar_charts(df))\n",
    "    with radar_plot_output_specific: radar_plot_output_specific.clear_output(wait=True); display(create_radar_chart(df))\n",
    "    generate_button_specific.disabled=False\n",
    "    generate_button_specific.description=\"🚀 Generate Summaries\"\n",
    "\n",
    "def on_clear_specific(b):\n",
    "    global metrics_history_specific\n",
    "    metrics_history_specific=[]\n",
    "    summary_output_specific.clear_output()\n",
    "    metrics_table_output_specific.clear_output()\n",
    "    bar_plot_output_specific.clear_output()\n",
    "    radar_plot_output_specific.clear_output()\n",
    "    text_input_specific.value=\"\"\n",
    "\n",
    "summary_type_specific.observe(lambda change: update_checkboxes(change.new), names='value')\n",
    "generate_button_specific.on_click(on_generate_specific)\n",
    "clear_button_specific.on_click(on_clear_specific)\n",
    "\n",
    "input_box_specific = widgets.VBox([text_input_specific, summary_type_specific, widgets.Label(\"Select Models:\"), model_checkboxes_out, widgets.HBox([generate_button_specific, clear_button_specific])], layout=widgets.Layout(width='35%', padding='10px', border='1px solid lightgrey', border_radius='5px'))\n",
    "output_box_specific = widgets.VBox([summary_output_specific, output_accordion_specific], layout=widgets.Layout(width='65%', padding='10px'))\n",
    "app_specific = widgets.VBox([header_specific, widgets.HBox([input_box_specific, output_box_specific])])\n",
    "update_checkboxes('Abstractive')\n",
    "display(app_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH_jwRbSN6yl"
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 10. SAMPLE TEXTS\n",
    "# ========================\n",
    "\n",
    "sample_texts = {\n",
    "    \"News\": \"\"\"The global stock markets experienced a major downturn today as technology shares plunged amid fears of a global economic slowdown. Investors are closely monitoring central bank policies for signals about interest rate changes.\"\"\",\n",
    "    \"Science\": \"\"\"A team of astronomers has discovered a new exoplanet in the habitable zone of a nearby star. The planet, roughly the size of Earth, could have conditions suitable for liquid water and potentially life.\"\"\",\n",
    "    \"Health\": \"\"\"Researchers have found that a diet rich in fruits and vegetables can reduce the risk of chronic diseases. A balanced diet combined with regular exercise is key to long-term wellness.\"\"\",\n",
    "    \"Technology\": \"\"\"The latest smartphone release features a foldable display, enhanced AI-powered camera, and faster 5G connectivity. Experts predict it will set new trends in mobile technology.\"\"\",\n",
    "    \"Sports\": \"\"\"In an exciting football match, the underdog team managed a surprising victory against the reigning champions. Fans celebrated wildly as the final whistle blew.\"\"\",\n",
    "    \"Finance\": \"\"\"Cryptocurrency markets showed volatility today with Bitcoin prices dropping 5% after regulatory concerns surfaced. Traders are advised to exercise caution and diversify their portfolios.\"\"\",\n",
    "    \"Education\": \"\"\"Universities are increasingly offering online courses to adapt to digital learning trends. Students now have the flexibility to access high-quality education from anywhere.\"\"\",\n",
    "    \"Environment\": \"\"\"Climate change continues to impact global weather patterns. Scientists warn that urgent measures are needed to reduce greenhouse gas emissions and protect vulnerable ecosystems.\"\"\",\n",
    "    \"Entertainment\": \"\"\"The latest blockbuster movie broke box office records on opening weekend. Critics praised the performances and special effects, predicting a long run in theaters.\"\"\",\n",
    "    \"History\": \"\"\"Archaeologists have uncovered ancient ruins dating back 3,000 years. The site reveals insights into the daily lives, trade, and culture of civilizations that existed in that region.\"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__k-G8GQN95r"
   },
   "outputs": [],
   "source": [
    "# 11. TESTING SAMPLE TEXTS\n",
    "# ========================\n",
    "\n",
    "sample_dropdown = widgets.Dropdown(\n",
    "    options=list(sample_texts.keys()),\n",
    "    description=\"Select Text:\",\n",
    "    layout={'width':'300px'}\n",
    ")\n",
    "\n",
    "def load_sample(change):\n",
    "    selected = change['new']\n",
    "    text_input_all.value = sample_texts[selected]\n",
    "    text_input_specific.value = sample_texts[selected]\n",
    "\n",
    "sample_dropdown.observe(load_sample, names='value')\n",
    "display(sample_dropdown)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
